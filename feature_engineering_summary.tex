\subsection{Feature Engineering}

Our main technical contribution is to augment the baseline macroeconomic predictors with a structured set of economically motivated engineered features. The goal is to expose latent nonlinearities, temporal dependencies, and regime-related information in a form that can be systematically exploited by machine learning algorithms.

We construct five primary feature families:
\begin{itemize}
    \item \textbf{Temporal Dynamics}: Moving averages, rolling extrema (min/max), and distribution moments (skewness) calculated over 3, 6, and 12-month windows to capture short- and medium-term trends.
    \item \textbf{Momentum Measures}: Absolute and relative changes in variables across multiple horizons to identify acceleration or deceleration in economic indicators.
    \item \textbf{Volatility Regimes}: Rolling standard deviations, realized volatility proxies, and robust dispersion measures such as the Median Absolute Deviation (MAD) to reflect changing uncertainty levels.
    \item \textbf{Relative Deviations}: Z-score transformations and deviations from local trends (12-month) to identify over-extended or mean-reverting economic conditions.
    \item \textbf{Cross-Sectional Relationships}: Dynamic correlations and interactions between theoretically related variable pairs to capture shifting lead-lag relationships.
\end{itemize}

In total, the feature engineering pipeline transforms the 126 baseline FRED-MD predictors into an enriched dataset of 5,061 features (126 raw + 4,935 engineered variables). To avoid the combinatorial explosion often associated with lagging high-dimensional sets, we implement a ``smart embedding'' strategy that applies 4 lags only to the 126 raw indicators (creating 504 lagged features) while maintaining contemporaneous engineered features at lag 0. This results in a comprehensive pool of approximately 5,000 potential predictors (504 lagged raw + 4,500 current engineered).

To control for redundancy and the curse of dimensionality, we apply a rigorous three-stage filtering process: (i) elimination of near-constant features (variance $< 10^{-8}$), (ii) high-correlation pruning ($|\rho| > 0.95$), and (iii) low-variance screening (variance $< 0.001$). For the Hybrid RF-FE model, we additionally employ SelectKBest pre-screening to reduce the feature space to approximately 500 most informative predictors before applying the three-stage filter. Furthermore, we rely on model-specific regularization mechanisms, such as the recursive partitioning of tree-based models (Random Forest and XGBoost). Following these optimization steps, the effective number of predictors utilized in each rolling window typically falls within the 450--550 range, providing a parsimonious yet information-rich input set for inflation forecasting.
