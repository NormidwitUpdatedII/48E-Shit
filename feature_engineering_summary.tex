\subsection{Feature Engineering}

Our main technical contribution is to augment the baseline macroeconomic predictors with a structured set of economically motivated engineered features. The goal is to expose latent nonlinearities, temporal dependencies, and regime-related information in a form that can be systematically exploited by machine learning algorithms.

We construct five primary feature families:
\begin{itemize}
    \item \textbf{Temporal Dynamics}: Moving averages, rolling extrema (min/max), and distribution moments (skewness) calculated over 3, 6, 12, and 24-month windows to capture short- and long-term trends.
    \item \textbf{Momentum Measures}: Absolute and relative changes in variables across multiple horizons to identify acceleration or deceleration in economic indicators.
    \item \textbf{Volatility Regimes}: Rolling standard deviations, realized volatility proxies, and robust dispersion measures such as the Median Absolute Deviation (MAD) to reflect changing uncertainty levels.
    \item \textbf{Relative Deviations}: Z-score transformations and deviations from local trends (12-month) to identify over-extended or mean-reverting economic conditions.
    \item \textbf{Cross-Sectional Relationships}: Dynamic correlations and interactions between theoretically related variable pairs to capture shifting lead-lag relationships.
\end{itemize}

In total, the feature engineering pipeline transforms the 126 baseline predictors into an enriched space of approximately 4,410 engineered variables. To avoid the combinatorial explosion often associated with lagging high-dimensional sets, we implement a ``smart embedding'' strategy that focuses lags only on the raw indicators while maintaining contemporaneous engineered features. This results in a comprehensive pool of approximately 5,000 potential predictors.

To control for redundancy and the curse of dimensionality, we apply a rigorous three-stage filtering process: (i) elimination of near-constant features, (ii) high-correlation pruning ($|\rho| > 0.95$), and (iii) low-variance screening. Furthermore, we rely on model-specific regularization and selection mechanisms, such as LASSO-type shrinkage and the recursive partitioning of tree-based models (Random Forest and XGBoost). Following these optimization steps, the effective number of predictors utilized in each rolling window typically falls within the 450--550 range, providing a parsimonious yet information-rich input set for inflation forecasting.
