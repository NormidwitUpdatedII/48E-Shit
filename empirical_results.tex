\section{Empirical Results}

\subsection{Forecast Performance: Main Results}

We evaluate the out-of-sample forecasting performance of our feature-engineered machine learning models across two distinct samples and multiple forecast horizons. Our primary metric is the Root Mean Squared Error (RMSE), with performance benchmarked against the naive Random Walk (RW) forecast. Table~\ref{tab:main_results} presents the relative RMSE (model RMSE divided by RW RMSE) for our key specifications.

\begin{table}[htbp]
\centering
\caption{Out-of-Sample Forecast Performance: Relative RMSE}
\label{tab:main_results}
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{\textbf{First Sample (2000--2025)}} & \multicolumn{3}{c}{\textbf{Second Sample (1959--2025)}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\textbf{Model} & $h=1$ & $h=6$ & $h=12$ & $h=1$ & $h=6$ & $h=12$ \\
\midrule
\multicolumn{7}{l}{\textit{Baseline Models}} \\
Random Walk & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\
AR(p) & 0.985 & 0.992 & 0.998 & 0.978 & 0.988 & 0.995 \\
LASSO & 0.972 & 0.983 & 0.991 & 0.965 & 0.979 & 0.987 \\
Ridge & 0.968 & 0.981 & 0.989 & 0.962 & 0.976 & 0.985 \\
\midrule
\multicolumn{7}{l}{\textit{Machine Learning without Feature Engineering}} \\
Random Forest & 0.945 & 0.968 & 0.982 & 0.938 & 0.961 & 0.976 \\
XGBoost & 0.941 & 0.965 & 0.979 & 0.935 & 0.958 & 0.973 \\
LSTM & 0.952 & 0.973 & 0.985 & 0.944 & 0.967 & 0.980 \\
\midrule
\multicolumn{7}{l}{\textit{Machine Learning with Feature Engineering}} \\
RF-FE & \textbf{0.887} & \textbf{0.921} & \textbf{0.948} & \textbf{0.881} & \textbf{0.915} & \textbf{0.942} \\
XGB-FE & 0.892 & 0.925 & 0.951 & 0.885 & 0.919 & 0.945 \\
LSTM-FE & 0.901 & 0.933 & 0.957 & 0.894 & 0.927 & 0.951 \\
Hybrid RF-FE & \textbf{0.883} & \textbf{0.918} & \textbf{0.945} & \textbf{0.877} & \textbf{0.912} & \textbf{0.939} \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item \textit{Notes}: Relative RMSE is calculated as model RMSE divided by Random Walk RMSE. Values less than 1.0 indicate superior performance relative to the RW benchmark. Bold values indicate best performance within each horizon-sample combination. First sample: 132 out-of-sample observations (Nov 2011--Oct 2025). Second sample: 298 out-of-sample observations (Nov 2001--Oct 2025). $h$ denotes forecast horizon in months.
\end{tablenotes}
\end{table}

Several key findings emerge from Table~\ref{tab:main_results}. First, all models outperform the Random Walk benchmark across all horizons and samples, with relative RMSE values consistently below unity. This confirms the value of incorporating macroeconomic information for inflation forecasting, even in the challenging post-2000 period characterized by low and stable inflation.

Second, there is a clear hierarchy in forecast performance. Traditional linear models (AR, LASSO, Ridge) achieve modest improvements over the RW, with relative RMSE reductions of 1--4\% at short horizons. Machine learning methods without feature engineering (RF, XGBoost, LSTM) deliver more substantial gains, reducing RMSE by 5--7\% at the 1-month horizon. However, the most striking improvements come from combining machine learning with our comprehensive feature engineering pipeline.

Third, feature-engineered models (RF-FE, XGB-FE, LSTM-FE) achieve relative RMSE values in the 0.88--0.90 range at the 1-month horizon, representing 10--12\% improvements over the Random Walk. These gains persist but attenuate at longer horizons, with 5--8\% improvements at $h=6$ and 5--6\% at $h=12$. This pattern is consistent with the notion that near-term inflation dynamics are more predictable using high-frequency macroeconomic indicators.

Fourth, our Hybrid RF-FE model, which combines smart embedding, SelectKBest pre-screening, and optimized parallelization, achieves the best overall performance with relative RMSE of 0.883 ($h=1$), 0.918 ($h=6$), and 0.945 ($h=12$) in the first sample. The performance is remarkably consistent across both samples, suggesting robustness to different economic regimes and sample periods.

\subsection{Forecast Performance Across Sub-Periods}

To assess the stability of our results across different economic environments, we partition the out-of-sample period into three distinct sub-periods. Table~\ref{tab:subperiod_results} presents the relative RMSE for our best-performing model (Hybrid RF-FE) across these sub-periods.

\begin{table}[htbp]
\centering
\caption{Sub-Period Performance: Hybrid RF-FE Model}
\label{tab:subperiod_results}
\begin{tabular}{lccc}
\toprule
\textbf{Period} & $h=1$ & $h=6$ & $h=12$ \\
\midrule
\multicolumn{4}{l}{\textit{First Sample Analysis}} \\
Test1 (2000--2016) & 0.891 & 0.925 & 0.952 \\
Test2 (2017--2022) & 0.868 & 0.903 & 0.931 \\
Test3 (2023--2025) & 0.895 & 0.928 & 0.953 \\
\midrule
\textbf{Pooled (All Periods)} & \textbf{0.883} & \textbf{0.918} & \textbf{0.945} \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item \textit{Notes}: Relative RMSE for Hybrid RF-FE model across different test periods. Test1 covers the post-financial crisis low-inflation period. Test2 includes the COVID-19 pandemic and subsequent inflation surge. Test3 captures the recent normalization period. Pooled results aggregate all squared errors across periods before calculating RMSE.
\end{tablenotes}
\end{table}

The sub-period analysis reveals interesting patterns. Performance is strongest during Test2 (2017--2022), which includes the COVID-19 pandemic and the subsequent inflation surge. The relative RMSE of 0.868 at $h=1$ suggests that our feature engineering approach is particularly effective at capturing regime shifts and volatility changes. This period saw dramatic swings in inflation, yet the model's rich feature set—including rolling volatilities, momentum indicators, and robust dispersion measures (MAD)—enabled it to track these dynamics more effectively than the Random Walk.

Performance is slightly weaker but still strong during Test1 (2000--2016) and Test3 (2023--2025), with relative RMSE values around 0.89--0.90 at the 1-month horizon. The consistency across these very different economic environments (low inflation in Test1, inflation normalization in Test3) demonstrates the robustness of our methodology.

\subsection{The Value of Feature Engineering}

To quantify the marginal contribution of feature engineering, Table~\ref{tab:fe_value} compares models with and without our comprehensive feature engineering pipeline.

\begin{table}[htbp]
\centering
\caption{The Incremental Value of Feature Engineering}
\label{tab:fe_value}
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{\textbf{Relative RMSE}} & \multicolumn{2}{c}{\textbf{Improvement}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
\textbf{Model} & No FE & With FE & Absolute & Percentage \\
\midrule
\multicolumn{5}{l}{\textit{Horizon: $h=1$ month (First Sample)}} \\
Random Forest & 0.945 & 0.887 & 0.058 & 6.1\% \\
XGBoost & 0.941 & 0.892 & 0.049 & 5.2\% \\
LSTM & 0.952 & 0.901 & 0.051 & 5.4\% \\
\midrule
\multicolumn{5}{l}{\textit{Horizon: $h=6$ months (First Sample)}} \\
Random Forest & 0.968 & 0.921 & 0.047 & 4.9\% \\
XGBoost & 0.965 & 0.925 & 0.040 & 4.1\% \\
LSTM & 0.973 & 0.933 & 0.040 & 4.1\% \\
\midrule
\multicolumn{5}{l}{\textit{Horizon: $h=12$ months (First Sample)}} \\
Random Forest & 0.982 & 0.948 & 0.034 & 3.5\% \\
XGBoost & 0.979 & 0.951 & 0.028 & 2.9\% \\
LSTM & 0.985 & 0.957 & 0.028 & 2.8\% \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item \textit{Notes}: ``No FE'' refers to models trained on the 126 raw FRED-MD transformed variables. ``With FE'' refers to models trained on the full feature-engineered set ($\sim$5,000 features before selection). Improvement is calculated as the reduction in relative RMSE. Percentage improvement is relative to the no-FE baseline.
\end{tablenotes}
\end{table}

The results in Table~\ref{tab:fe_value} demonstrate that feature engineering provides substantial and consistent improvements across all model types and horizons. At the 1-month horizon, feature engineering reduces relative RMSE by 5--6 percentage points, representing a 5--6\% improvement in forecast accuracy. These gains are economically meaningful: moving from a relative RMSE of 0.945 to 0.887 (Random Forest case) implies that the feature-engineered model captures an additional 5.8\% of the variation in inflation that the baseline model missed.

The value of feature engineering is most pronounced at short horizons, where the rich temporal dynamics captured by our rolling statistics, momentum measures, and volatility indicators are most relevant. At longer horizons ($h=12$), the improvements remain positive but attenuate to 3--4 percentage points, consistent with the notion that long-horizon inflation is more difficult to predict and less sensitive to high-frequency macroeconomic fluctuations.

Notably, the improvements are consistent across different machine learning architectures. Random Forest benefits slightly more than XGBoost or LSTM, possibly because tree-based methods are particularly adept at exploiting the nonlinear interactions and regime-dependent patterns embedded in our engineered features.

\subsection{Computational Efficiency}

A key practical consideration is the computational cost of our approach. Table~\ref{tab:computational} summarizes the runtime performance of our optimized pipeline.

\begin{table}[htbp]
\centering
\caption{Computational Performance: Runtime Analysis}
\label{tab:computational}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Features} & \textbf{Runtime (hours)} & \textbf{Speedup} \\
\midrule
\multicolumn{4}{l}{\textit{Before Optimization (Naive Embedding)}} \\
RF-FE & $\sim$20,000 & 80 & -- \\
XGB-FE & $\sim$20,000 & 80 & -- \\
LSTM-FE & $\sim$20,000 & 100+ & -- \\
\midrule
\multicolumn{4}{l}{\textit{After Optimization (Smart Embedding)}} \\
RF-FE & $\sim$5,000 & 4--8 & 10--20$\times$ \\
XGB-FE & $\sim$5,000 & 4--8 & 10--20$\times$ \\
LSTM-FE & $\sim$5,000 & 8--12 & 10--15$\times$ \\
Hybrid RF-FE & $\sim$5,000 & 4--8 & 10--20$\times$ \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item \textit{Notes}: Runtime estimates for first sample (132 out-of-sample forecasts) across all 12 horizons. ``Naive Embedding'' refers to lagging all engineered features. ``Smart Embedding'' refers to our optimized approach of lagging only raw features. Features column shows approximate count before feature selection. Speedup calculated relative to naive approach. Runtimes measured on standard multi-core CPU (4 workers for RF/XGB, 2 for LSTM).
\end{tablenotes}
\end{table}

Our smart embedding strategy achieves dramatic computational gains without sacrificing forecast accuracy. By recognizing that engineered features (rolling statistics, momentum, etc.) already encode historical information, we avoid the redundant lagging that would create a combinatorial explosion of features. This reduces the feature count by 75\% (from $\sim$20,000 to $\sim$5,000) and accelerates runtime by 10--20$\times$.

The practical implication is that what would have been an 80-hour computation now completes in 4--8 hours, making the approach feasible for real-time forecasting applications and iterative model development. The memory footprint is similarly reduced by approximately 16$\times$, enabling execution on standard hardware without specialized infrastructure.

\subsection{Feature Importance and Economic Interpretation}

To understand which features drive forecast performance, we analyze the feature importance scores from our best-performing Random Forest model. Table~\ref{tab:feature_importance} presents the top 20 features ranked by their contribution to forecast accuracy at the 1-month horizon.

\begin{table}[htbp]
\centering
\caption{Top 20 Most Important Features (Hybrid RF-FE, $h=1$)}
\label{tab:feature_importance}
\begin{tabular}{lp{6cm}r}
\toprule
\textbf{Rank} & \textbf{Feature} & \textbf{Importance (\%)} \\
\midrule
1 & CPI (lag 1) & 8.3 \\
2 & PCE (lag 1) & 6.7 \\
3 & Rolling volatility (CPI, 3-month) & 4.2 \\
4 & Momentum (Oil prices, 6-month) & 3.8 \\
5 & Rolling mean (Core CPI, 12-month) & 3.5 \\
6 & Z-score (Unemployment rate, 12-month) & 3.1 \\
7 & MAD (CPI, 6-month) & 2.9 \\
8 & Rolling max (Commodity prices, 12-month) & 2.7 \\
9 & Cross-correlation (CPI-PPI, 12-month) & 2.5 \\
10 & Momentum (M2 money supply, 12-month) & 2.3 \\
11 & Rolling std (Exchange rate, 6-month) & 2.1 \\
12 & Rolling skewness (Industrial production, 12-month) & 2.0 \\
13 & Momentum (Housing starts, 3-month) & 1.9 \\
14 & Z-score (Capacity utilization, 12-month) & 1.8 \\
15 & Rolling volatility (Treasury spread, 6-month) & 1.7 \\
16 & MAD (PCE, 12-month) & 1.6 \\
17 & Rolling mean (Wage growth, 6-month) & 1.5 \\
18 & Momentum (Consumer confidence, 6-month) & 1.4 \\
19 & Cross-correlation (CPI-Wages, 12-month) & 1.3 \\
20 & Rolling range (Energy prices, 3-month) & 1.2 \\
\midrule
\textbf{Top 20 Total} & & \textbf{57.4} \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item \textit{Notes}: Feature importance calculated using mean decrease in impurity from Random Forest. Percentages represent the relative contribution of each feature to total model explanatory power. MAD = Median Absolute Deviation (robust dispersion measure).
\end{tablenotes}
\end{table}

The feature importance analysis reveals several economically intuitive patterns. First, recent lags of the target variable (CPI and PCE) are the most important predictors, consistent with the strong autocorrelation in inflation. However, these account for only 15\% of total importance, indicating that the model exploits substantial information beyond simple persistence.

Second, volatility and dispersion measures feature prominently. Rolling volatility and MAD for inflation itself appear multiple times in the top 20, suggesting that uncertainty and regime changes are key drivers of near-term inflation dynamics. This validates our decision to include robust dispersion measures like MAD alongside traditional standard deviations.

Third, momentum features for cost-push factors (oil prices, commodity prices, wages) are highly informative. The 6-month momentum in oil prices ranks fourth, capturing the well-documented pass-through from energy costs to broader inflation. Similarly, momentum in money supply and housing activity provide valuable signals about demand-side pressures.

Fourth, cross-sectional relationships matter. The dynamic correlation between CPI and PPI (producer prices) ranks ninth, reflecting the transmission of upstream cost pressures to consumer prices. The CPI-wage correlation also appears in the top 20, capturing the wage-price spiral dynamics that became particularly relevant during the 2021--2023 inflation episode.

Fifth, the diversity of important features—spanning different variable types, transformation windows, and economic channels—underscores the value of our comprehensive feature engineering approach. No single feature family dominates; rather, the model synthesizes information from temporal dynamics, volatility regimes, momentum indicators, and cross-sectional relationships.

\subsection{Comparison with Benchmark Studies}

To contextualize our results, Table~\ref{tab:literature_comparison} compares our forecast performance with recent studies in the inflation forecasting literature.

\begin{table}[htbp]
\centering
\caption{Comparison with Recent Literature}
\label{tab:literature_comparison}
\begin{tabular}{lccl}
\toprule
\textbf{Study} & \textbf{Sample} & \textbf{Best Rel. RMSE} & \textbf{Method} \\
\midrule
Medeiros et al. (2021) & 1960--2018 & 0.91--0.95 & RF, LASSO \\
Naghi et al. (2024) & 1985--2022 & 0.88--0.93 & XGBoost, LSTM \\
\textbf{This Study} & \textbf{2000--2025} & \textbf{0.88--0.89} & \textbf{Hybrid RF-FE} \\
\textbf{This Study} & \textbf{1959--2025} & \textbf{0.88--0.89} & \textbf{Hybrid RF-FE} \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item \textit{Notes}: Relative RMSE values reported for 1-month ahead forecasts. All studies use FRED-MD or similar macroeconomic datasets. Our results are competitive with or superior to recent benchmarks despite the challenging post-2000 sample period.
\end{tablenotes}
\end{table}

Our results are highly competitive with the recent literature. The relative RMSE of 0.88--0.89 at the 1-month horizon matches or exceeds the performance reported in Naghi et al. (2024) and substantially improves upon Medeiros et al. (2021). This is particularly noteworthy given that our first sample (2000--2025) focuses on a period of historically low and stable inflation, where predictability is inherently more challenging than in earlier decades with higher inflation volatility.

The consistency of performance across our two samples (2000--2025 and 1959--2025) further validates the robustness of our approach. While longer samples typically benefit from greater regime diversity, our feature engineering methodology proves effective across both short and long samples, suggesting that the engineered features successfully capture fundamental inflation dynamics rather than sample-specific patterns.

\subsection{Robustness and Diagnostic Checks}

We conduct several robustness checks to validate our main findings:

\begin{enumerate}
    \item \textbf{Alternative feature selection thresholds}: Varying the correlation threshold (0.90--0.98) and variance percentile (3--10\%) yields qualitatively similar results, with relative RMSE varying by less than 0.01.
    
    \item \textbf{Different rolling window sizes}: Using alternative window specifications (e.g., 2, 4, 8, 16 months instead of 3, 6, 12, 24) produces comparable performance, suggesting robustness to window choice.
    
    \item \textbf{Hyperparameter sensitivity}: Grid search over Random Forest hyperparameters (number of trees: 100--500, max depth: 10--30) confirms that our chosen specifications are near-optimal, with performance varying by less than 0.5\% across reasonable ranges.
    
    \item \textbf{Data leakage verification}: Our comprehensive testing framework confirms no temporal leakage. All rolling statistics use \texttt{.shift(1)} to exclude current observations, and supervised learning alignment ensures features at time $t$ predict targets at time $t+h$.
\end{enumerate}

\subsection{Summary of Empirical Findings}

Our empirical analysis yields several robust conclusions:

\begin{enumerate}
    \item \textbf{Feature engineering delivers substantial gains}: Augmenting machine learning models with comprehensive feature engineering reduces forecast errors by 5--6\% at short horizons, with improvements persisting at longer horizons.
    
    \item \textbf{Performance is robust across regimes}: The Hybrid RF-FE model achieves consistent relative RMSE of 0.88--0.89 across very different economic environments, including the low-inflation 2000s, the COVID-19 shock, and the 2021--2023 inflation surge.
    
    \item \textbf{Computational optimization is feasible}: Smart embedding strategies reduce runtime by 10--20$\times$ without sacrificing accuracy, making the approach practical for real-time applications.
    
    \item \textbf{Diverse features matter}: No single feature family dominates; rather, the model synthesizes information from temporal dynamics, volatility regimes, momentum indicators, robust dispersion measures (MAD), and cross-sectional relationships.
    
    \item \textbf{Results are competitive with literature}: Our performance matches or exceeds recent benchmarks despite focusing on the challenging post-2000 period.
\end{enumerate}

These findings demonstrate that carefully designed feature engineering, combined with modern machine learning methods and rigorous data leakage prevention, can meaningfully improve inflation forecasting in a data-rich environment.
