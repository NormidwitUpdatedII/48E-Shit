# Naghiayik Python - Inflation Forecasting with Machine Learning

A comprehensive Python implementation of the R codebase from **"Forecasting Inflation in a Data-Rich Environment: The Benefits of Machine Learning Methods"** by Medeiros, Vasconcelos, Veiga and Zilberman (2018).

## ğŸ“– Overview

This repository contains Python implementations of various machine learning and econometric methods for inflation forecasting using rolling window evaluation. The original R code has been faithfully converted to Python while maintaining the same structure and methodology.

### Key Features
- **17+ forecasting methods** implemented
- **Rolling window evaluation** with configurable forecast horizons
- **Two sample periods** for analysis (first-sample, second-sample)
- **Modular design** with separate function and run modules
- **Comprehensive error metrics** (RMSE, MAE, MAPE)

## ğŸ“ Project Structure

```
Naghiayik-python/
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ utils.py                  # Shared utility functions
â”œâ”€â”€ test_all.py              # Comprehensive test suite
â”œâ”€â”€ first_sample/            # First sample period analysis
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ functions/           # Model function implementations
â”‚   â”‚   â”œâ”€â”€ func_ar.py       # Autoregressive models
â”‚   â”‚   â”œâ”€â”€ func_lasso.py    # LASSO regression
â”‚   â”‚   â”œâ”€â”€ func_rf.py       # Random Forest
â”‚   â”‚   â”œâ”€â”€ func_xgb.py      # XGBoost
â”‚   â”‚   â”œâ”€â”€ func_nn.py       # Neural Networks
â”‚   â”‚   â”œâ”€â”€ func_boosting.py # Gradient Boosting
â”‚   â”‚   â”œâ”€â”€ func_bag.py      # Bagging
â”‚   â”‚   â”œâ”€â”€ func_csr.py      # Complete Subset Regression
â”‚   â”‚   â”œâ”€â”€ func_fact.py     # Factor Models
â”‚   â”‚   â”œâ”€â”€ func_tfact.py    # Targeted Factor Models
â”‚   â”‚   â”œâ”€â”€ func_scad.py     # SCAD Penalized Regression
â”‚   â”‚   â”œâ”€â”€ func_jn.py       # Jackknife
â”‚   â”‚   â”œâ”€â”€ func_rfols.py    # Random Forest OLS
â”‚   â”‚   â”œâ”€â”€ func_lbvar.py    # Large Bayesian VAR
â”‚   â”‚   â”œâ”€â”€ func_ucsv.py     # Unobserved Components SV
â”‚   â”‚   â”œâ”€â”€ func_polilasso.py    # Polynomial LASSO
â”‚   â”‚   â”œâ”€â”€ func_adalassorf.py   # Adaptive LASSO RF
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ run/                 # Execution scripts
â”‚       â”œâ”€â”€ ar.py            # Run AR models
â”‚       â”œâ”€â”€ lasso.py         # Run LASSO
â”‚       â”œâ”€â”€ adalasso.py      # Run Adaptive LASSO
â”‚       â”œâ”€â”€ elasticnet.py    # Run Elastic Net
â”‚       â”œâ”€â”€ ridge.py         # Run Ridge Regression
â”‚       â”œâ”€â”€ rf.py            # Run Random Forest
â”‚       â”œâ”€â”€ xgb.py           # Run XGBoost
â”‚       â”œâ”€â”€ nn.py            # Run Neural Networks
â”‚       â”œâ”€â”€ boosting.py      # Run Boosting
â”‚       â”œâ”€â”€ bagging.py       # Run Bagging
â”‚       â”œâ”€â”€ csr.py           # Run CSR
â”‚       â”œâ”€â”€ factors.py       # Run Factor Models
â”‚       â”œâ”€â”€ tfactors.py      # Run Targeted Factors
â”‚       â”œâ”€â”€ scad.py          # Run SCAD
â”‚       â”œâ”€â”€ jackknife.py     # Run Jackknife
â”‚       â”œâ”€â”€ rfols.py         # Run RF-OLS
â”‚       â”œâ”€â”€ lbvar.py         # Run LBVAR
â”‚       â”œâ”€â”€ ucsv.py          # Run UC-SV
â”‚       â””â”€â”€ __init__.py
â””â”€â”€ second_sample/           # Second sample period analysis
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ functions/           # Same structure as first_sample
    â”‚   â”œâ”€â”€ func_flasso.py   # Forecast LASSO (unique)
    â”‚   â”œâ”€â”€ func_rflasso.py  # RF LASSO (unique)
    â”‚   â””â”€â”€ ... (same as first_sample)
    â””â”€â”€ run/
        â”œâ”€â”€ cm.py            # Combination Methods (unique)
        â”œâ”€â”€ fadalasso.py     # Forecast Adaptive LASSO
        â”œâ”€â”€ rflasso.py       # RF LASSO
        â”œâ”€â”€ rlasso.py        # Robust LASSO
        â””â”€â”€ ... (same as first_sample)
```

## ğŸ”§ Installation

### Prerequisites
- Python 3.10+ (tested with Python 3.14)
- pip package manager

### Install Dependencies

```bash
cd Naghiayik-python
pip install -r requirements.txt
```

### Required Packages
| Package | Version | Purpose |
|---------|---------|---------|
| numpy | â‰¥1.24.0 | Numerical computing |
| pandas | â‰¥2.0.0 | Data manipulation |
| scipy | â‰¥1.10.0 | Scientific computing |
| scikit-learn | â‰¥1.3.0 | Machine learning algorithms |
| xgboost | â‰¥2.0.0 | Gradient boosting |
| pyreadr | â‰¥0.5.0 | Load R data files (.rda, .RData) |
| matplotlib | â‰¥3.7.0 | Visualization (optional) |
| statsmodels | â‰¥0.14.0 | Statistical models (optional) |

## ğŸš€ Usage

### Quick Start

```python
# Load data and run AR model
import pyreadr
from first_sample.functions.func_ar import ar_rolling_window

# Load R data file
result = pyreadr.read_r('first-sample/rawdata.rda')
Y = result['dados'].values

# Run AR(1) with rolling window
nprev = 132  # Number of out-of-sample forecasts
forecasts = ar_rolling_window(Y, nprev, indice=1, lag=1, model_type="fixed")
```

### Running Individual Models

```python
# Example: Run LASSO model
from first_sample.functions.func_lasso import lasso_rolling_window

result = lasso_rolling_window(Y, nprev, indice=1, alpha=1.0)
print(f"RMSE: {result['rmse']}")
```

### Running Complete Experiments

```python
# Run the complete AR experiment (from run scripts)
from first_sample.run.ar import ar_main
results = ar_main(data_path='first-sample/rawdata.rda', nprev=132)
```

## ğŸ“Š Methods Implemented

### Linear Methods
| Method | Description | Function |
|--------|-------------|----------|
| AR | Autoregressive models (lags 1-12) | `func_ar.py` |
| Ridge | Ridge regression | `func_lasso.py` |
| LASSO | Least Absolute Shrinkage | `func_lasso.py` |
| Elastic Net | L1 + L2 regularization | `func_lasso.py` |
| Adaptive LASSO | Weighted LASSO | `func_lasso.py` |
| SCAD | Smoothly Clipped Absolute Deviation | `func_scad.py` |

### Machine Learning Methods
| Method | Description | Function |
|--------|-------------|----------|
| Random Forest | Ensemble of decision trees | `func_rf.py` |
| XGBoost | Extreme Gradient Boosting | `func_xgb.py` |
| Neural Network | Feedforward neural network | `func_nn.py` |
| Gradient Boosting | Sequential ensemble | `func_boosting.py` |
| Bagging | Bootstrap aggregating | `func_bag.py` |

### Factor and Dimension Reduction Methods
| Method | Description | Function |
|--------|-------------|----------|
| Factor Models | Principal component regression | `func_fact.py` |
| Targeted Factors | Targeted principal components | `func_tfact.py` |
| CSR | Complete Subset Regression | `func_csr.py` |
| RF-OLS | Random Forest variable selection + OLS | `func_rfols.py` |

### Bayesian and Other Methods
| Method | Description | Function |
|--------|-------------|----------|
| LBVAR | Large Bayesian VAR | `func_lbvar.py` |
| UC-SV | Unobserved Components with Stochastic Volatility | `func_ucsv.py` |
| Jackknife | Jackknife Model Averaging | `func_jn.py` |

## ğŸ“ˆ Rolling Window Evaluation

The forecasting is performed using an expanding or rolling window approach:

```
Training Period     |  Test Point
[==================]|[*]
     â””â”€ Estimate    â””â”€ Forecast

nprev = 132 (132 out-of-sample forecast points)
```

### Parameters
- **nprev**: Number of out-of-sample forecasts (default: 132)
- **indice**: Target variable index (1 = CPI, 2 = PCE)
- **lag**: Number of lags to include
- **model_type**: "fixed" (fixed window) or "bic" (BIC selection)

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
cd Naghiayik-python
python test_all.py
```

Expected output:
```
============================================================
NAGHIAYIK PYTHON TEST SUITE
============================================================
[1] SYNTAX CHECK
----------------------------------------
  OK: utils.py
  OK: first_sample/functions/func_ar.py
  ... (88 files)

Syntax Results: 88 passed, 0 failed
============================================================
```

## ğŸ“š Data Format

### Input Data Structure
The input data (`rawdata.rda` or `rawdata.RData`) should contain:
- A DataFrame/matrix with time series observations
- First column: target variable (inflation measure)
- Remaining columns: predictor variables (macroeconomic indicators)

### Output Format
Each model returns a dictionary containing:
```python
{
    'forecasts': np.array,    # Out-of-sample predictions
    'actual': np.array,       # Actual values
    'rmse': float,            # Root Mean Square Error
    'mae': float,             # Mean Absolute Error
    'model_info': dict        # Model-specific information
}
```

## ğŸ”¬ Methodology

### Forecast Horizons
- h = 1: One-step ahead forecast
- h = 3: Three-month ahead
- h = 6: Six-month ahead
- h = 12: Twelve-month ahead (annual)

### Model Selection
- **BIC**: Bayesian Information Criterion for lag selection
- **Cross-Validation**: For regularization parameters
- **Grid Search**: For hyperparameter tuning (ML methods)

## ğŸ“– References

1. Medeiros, M. C., Vasconcelos, G., Veiga, A., & Zilberman, E. (2018). **Forecasting Inflation in a Data-Rich Environment: The Benefits of Machine Learning Methods.** Journal of Business & Economic Statistics.

2. Original R Implementation: [HDeconometrics](https://github.com/gabrielrvsc/HDeconometrics)


