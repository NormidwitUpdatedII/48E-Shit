% Feature Engineering Section for Overleaf
% Copy this into your Overleaf document

\section{Feature Engineering Methodology}

\subsection{Motivation}

Traditional macroeconomic forecasting models often rely on a limited set of predictors, potentially missing valuable information contained in the broader economic landscape. To address this limitation, we implement a comprehensive feature engineering pipeline that transforms the raw FRED-MD dataset into an enriched feature space capturing temporal dynamics, cross-sectional relationships, and nonlinear patterns.

Our feature engineering approach is motivated by three key observations:
\begin{enumerate}
    \item \textbf{Temporal Dependencies}: Economic variables exhibit strong autocorrelation and momentum effects that can improve forecast accuracy.
    \item \textbf{Cross-Sectional Information}: Relationships between different macroeconomic indicators contain predictive power for inflation dynamics.
    \item \textbf{Nonlinear Patterns}: Economic relationships are often nonlinear, requiring transformations beyond simple lags.
\end{enumerate}

\subsection{Data Pipeline}

Our feature engineering pipeline consists of two main stages:

\subsubsection{Stage 1: Stationarity Transformations (FRED-MD)}

Following \citet{mccracken2016fred}, we apply appropriate transformations to ensure stationarity:
\begin{itemize}
    \item \textbf{Level} (Code 1): No transformation for stationary series
    \item \textbf{First Difference} (Code 2): $\Delta x_t = x_t - x_{t-1}$
    \item \textbf{Second Difference} (Code 3): $\Delta^2 x_t = \Delta x_t - \Delta x_{t-1}$
    \item \textbf{Log} (Code 4): $\log(x_t)$
    \item \textbf{Log Difference} (Code 5): $\Delta \log(x_t)$
    \item \textbf{Log Second Difference} (Code 6): $\Delta^2 \log(x_t)$
\end{itemize}

Starting with 126 raw macroeconomic variables from FRED-MD, these transformations yield 126 stationary predictors.

\subsubsection{Stage 2: Advanced Feature Engineering}

We apply a comprehensive set of feature engineering techniques to capture temporal and cross-sectional dynamics:

\paragraph{Rolling Statistics (4 windows: 3, 6, 12, 24 months)}
For each variable $x_t$ and window size $w$:
\begin{align}
    \text{Mean}_w(x_t) &= \frac{1}{w}\sum_{i=1}^{w} x_{t-i} \\
    \text{Std}_w(x_t) &= \sqrt{\frac{1}{w}\sum_{i=1}^{w} (x_{t-i} - \text{Mean}_w(x_t))^2} \\
    \text{Max}_w(x_t) &= \max_{i=1,\ldots,w} x_{t-i} \\
    \text{Min}_w(x_t) &= \min_{i=1,\ldots,w} x_{t-i}
\end{align}

\textbf{Data Leakage Prevention}: All rolling statistics use \texttt{.shift(1)} to exclude the current observation and \texttt{min\_periods=window} to ensure sufficient historical data.

\paragraph{Momentum Features (4 windows: 3, 6, 12, 24 months)}
\begin{equation}
    \text{Momentum}_w(x_t) = x_t - x_{t-w}
\end{equation}

\paragraph{Volatility Features (4 windows: 3, 6, 12, 24 months)}
\begin{equation}
    \text{Volatility}_w(x_t) = \text{Std}_w(x_t)
\end{equation}

\paragraph{Z-Score Normalization (1 window: 12 months)}
\begin{equation}
    \text{Z-Score}_{12}(x_t) = \frac{x_t - \text{Mean}_{12}(x_{t-1})}{\text{Std}_{12}(x_{t-1})}
\end{equation}

\paragraph{Cross-Sectional Correlation (1 window: 12 months)}
For each pair of variables $(x_t, y_t)$:
\begin{equation}
    \text{Corr}_{12}(x_t, y_t) = \frac{\text{Cov}_{12}(x_{t-1}, y_{t-1})}{\text{Std}_{12}(x_{t-1}) \cdot \text{Std}_{12}(y_{t-1})}
\end{equation}

\subsection{Feature Space Dimensions}

Table~\ref{tab:feature_counts} summarizes the feature space at each stage of our pipeline.

\begin{table}[htbp]
\centering
\caption{Feature Engineering Pipeline: Variable Counts}
\label{tab:feature_counts}
\begin{tabular}{lrr}
\toprule
\textbf{Stage} & \textbf{Features} & \textbf{Description} \\
\midrule
Raw FRED-MD & 126 & Original macroeconomic variables \\
Stationarity Transforms & 126 & After FRED-MD transformations \\
Rolling Statistics & 2,016 & 126 vars $\times$ 4 stats $\times$ 4 windows \\
Momentum & 504 & 126 vars $\times$ 4 windows \\
Volatility & 504 & 126 vars $\times$ 4 windows \\
Z-Scores & 126 & 126 vars $\times$ 1 window \\
Cross-Sectional Corr. & 1,260 & Selected pairs $\times$ 1 window \\
\midrule
\textbf{Total Engineered} & \textbf{4,410} & Before lagging \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Smart Embedding Strategy}

\subsubsection{The Feature Explosion Problem}

A naive approach would lag all 4,410 engineered features by $h$ periods (where $h$ is the forecast horizon). This creates a \textit{feature explosion}:
\begin{equation}
    \text{Naive Features} = 4,410 \times (4 + h) \approx 20,000 \text{ features}
\end{equation}

This approach has critical flaws:
\begin{enumerate}
    \item \textbf{Redundancy}: Rolling statistics already incorporate historical information. A 12-month rolling mean at time $t$ uses data from $[t-12, t-1]$. Lagging it by 1 month gives $[t-13, t-2]$, which is 92\% redundant.
    \item \textbf{Computational Cost}: With 20,000 features, the correlation matrix requires $20,000^2 = 400$ million computations.
    \item \textbf{Runtime}: Feature selection and model training become prohibitively slow (80+ hours).
\end{enumerate}

\subsubsection{Our Solution: Smart Embedding}

We implement a \textit{smart embedding strategy} that separates raw feature lagging from feature engineering:

\begin{algorithm}[H]
\caption{Smart Embedding for Feature Engineering}
\begin{algorithmic}[1]
\State \textbf{Input:} Raw data $Y \in \mathbb{R}^{T \times 126}$, forecast horizon $h$
\State \textbf{Step 1:} Create lags of \textit{raw} features only
\State $X_{\text{raw}} \gets \text{embed}(Y, 4)$ \Comment{126 vars $\times$ 4 lags = 504 features}
\State \textbf{Step 2:} Engineer features on \textit{current} data (no lags)
\State $X_{\text{eng}} \gets \text{FeatureEngineer}(Y)$ \Comment{4,410 current features}
\State \textbf{Step 3:} Select appropriate lags for forecast horizon $h$
\If{$h = 1$}
    \State $X_{\text{lagged}} \gets X_{\text{raw}}$
\Else
    \State $X_{\text{lagged}} \gets X_{\text{raw}}[:, 126(h-1):126(h-1)+504]$
\EndIf
\State \textbf{Step 4:} Combine lagged raw + current engineered
\State $X \gets [X_{\text{lagged}} \mid X_{\text{eng}}]$ \Comment{504 + 4,410 = 4,914 features}
\State \textbf{Output:} Feature matrix $X \in \mathbb{R}^{T \times 4,914}$
\end{algorithmic}
\end{algorithm}

This approach reduces the feature count by 75\% (from $\sim$20,000 to $\sim$5,000) while preserving all relevant information.

\subsection{Feature Selection Pipeline}

Given the high-dimensional feature space, we implement a three-stage feature selection process:

\subsubsection{Stage 1: Constant Variance Removal}
Remove features with near-zero variance ($\sigma^2 < 10^{-6}$):
\begin{equation}
    \mathcal{F}_1 = \{j : \text{Var}(X_j) \geq 10^{-6}\}
\end{equation}

\subsubsection{Stage 2: Correlation-Based Filtering}
For highly correlated feature pairs ($|\rho| > 0.95$), retain the feature with higher variance:
\begin{equation}
    \text{If } |\text{Corr}(X_i, X_j)| > 0.95 \text{ and } \text{Var}(X_i) > \text{Var}(X_j), \text{ remove } X_j
\end{equation}

\subsubsection{Stage 3: Low Variance Removal}
Remove features in the bottom 5\% of variance distribution:
\begin{equation}
    \mathcal{F}_3 = \{j : \text{Var}(X_j) > Q_{0.05}(\text{Var}(X))\}
\end{equation}

\subsubsection{Hybrid Models: SelectKBest Pre-screening}

For our hybrid RF-FE model, we add an additional pre-screening step using statistical significance:
\begin{equation}
    F_j = \frac{\text{Var}(\mathbb{E}[y|X_j])}{\mathbb{E}[\text{Var}(y|X_j)]}
\end{equation}

We select the top 500 features with highest $F$-statistics before applying the three-stage selection, further improving computational efficiency.

\subsection{Computational Optimization}

\subsubsection{Parallelization Strategy}

We implement parallel processing using \texttt{joblib} with controlled parallelization:
\begin{itemize}
    \item \textbf{Random Forest \& XGBoost}: \texttt{n\_jobs=4} (prevents out-of-memory errors)
    \item \textbf{LSTM}: \texttt{n\_jobs=2} (GPU memory constraints)
    \item \textbf{Batch Processing}: Process forecasts in batches of 20 for memory management
\end{itemize}

\subsubsection{Memory Efficiency}

\begin{itemize}
    \item \textbf{Data Type Optimization}: Use \texttt{float32} instead of \texttt{float64} (50\% memory reduction)
    \item \textbf{Incremental Processing}: Feature engineering applied within rolling windows
    \item \textbf{Garbage Collection}: Explicit memory cleanup between iterations
\end{itemize}

\subsection{Performance Comparison}

Table~\ref{tab:performance} compares the naive and smart embedding approaches.

\begin{table}[htbp]
\centering
\caption{Performance Comparison: Naive vs. Smart Embedding}
\label{tab:performance}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Naive Approach} & \textbf{Smart Embedding} \\
\midrule
Total Features & $\sim$20,000 & $\sim$5,000 \\
Feature Reduction & -- & 75\% \\
Correlation Matrix Size & 400M elements & 25M elements \\
Memory Usage & $\sim$16 GB & $\sim$1 GB \\
Runtime (RF-FE) & 80 hours & 4--8 hours \\
Speedup & 1$\times$ & 10--20$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Leakage Prevention}

We implement strict temporal separation to prevent data leakage:

\begin{enumerate}
    \item \textbf{Rolling Statistics}: All calculations use \texttt{.shift(1)} to exclude current observation
    \item \textbf{Minimum Periods}: Set \texttt{min\_periods=window} to ensure sufficient historical data
    \item \textbf{No Backward Fill}: Use forward fill and zero-fill only; no \texttt{bfill()}
    \item \textbf{Expanding Window}: Training data strictly precedes forecast date
    \item \textbf{Feature Engineering Timing}: Applied separately within each rolling window
\end{enumerate}

\subsection{Implementation Details}

Our feature engineering pipeline is implemented in Python using:
\begin{itemize}
    \item \textbf{Core Libraries}: NumPy 1.24+, Pandas 2.0+
    \item \textbf{Machine Learning}: scikit-learn 1.3+
    \item \textbf{Deep Learning}: TensorFlow 2.13+ (for LSTM models)
    \item \textbf{Parallelization}: joblib 1.3+
    \item \textbf{Code Availability}: \url{https://github.com/NormidwitUpdatedII/48E-Shit}
\end{itemize}

All code is available in our GitHub repository with comprehensive documentation and reproducible examples.

\subsection{Summary}

Our feature engineering methodology transforms 126 raw macroeconomic variables into a rich feature space of approximately 5,000 predictors through:
\begin{itemize}
    \item Comprehensive temporal feature extraction (rolling statistics, momentum, volatility)
    \item Cross-sectional relationship modeling (correlations, z-scores)
    \item Smart embedding strategy to avoid feature explosion
    \item Rigorous three-stage feature selection
    \item Strict data leakage prevention
    \item Computational optimization (parallelization, memory efficiency)
\end{itemize}

This approach achieves a 10--20$\times$ speedup while maintaining forecast accuracy and ensuring temporal validity of all features.

\section{Experimental Design and Model Configurations}

\subsection{Data Samples}

We conduct our analysis using two distinct samples to ensure robustness:

\subsubsection{First Sample (2000--2025)}

\begin{itemize}
    \item \textbf{Total Observations}: 502 months (January 2000 -- October 2025)
    \item \textbf{Training Period}: 370 months (January 2000 -- October 2011)
    \item \textbf{Out-of-Sample Period}: 132 months (November 2011 -- October 2025)
    \item \textbf{Evaluation}: \texttt{nprev = 132}
    \item \textbf{Rationale}: Focuses on recent economic dynamics, including post-2008 financial crisis and COVID-19 pandemic
\end{itemize}

\subsubsection{Second Sample (1959--2025)}

\begin{itemize}
    \item \textbf{Total Observations}: 800 months (January 1959 -- October 2025)
    \item \textbf{Training Period}: 502 months (January 1959 -- October 2001)
    \item \textbf{Out-of-Sample Period}: 298 months (November 2001 -- October 2025)
    \item \textbf{Evaluation}: \texttt{nprev = 298}
    \item \textbf{Outlier Handling}: Includes dummy variable for COVID-19 period (March--April 2020)
    \item \textbf{Rationale}: Provides longer historical perspective spanning multiple business cycles
\end{itemize}

\subsection{Forecast Horizons}

We evaluate forecasts at 12 different horizons:
\begin{equation}
    h \in \{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\} \text{ months}
\end{equation}

This allows us to assess both short-term (1--3 months) and medium-term (6--12 months) forecast performance.

\subsection{Target Variables}

We forecast two key inflation measures:
\begin{enumerate}
    \item \textbf{CPI}: Consumer Price Index for All Urban Consumers (CPIAUCSL)
    \item \textbf{PCE}: Personal Consumption Expenditures Price Index (PCEPI)
\end{enumerate}

Both variables are transformed to monthly growth rates following FRED-MD transformation codes.

\subsection{Model Configurations}

\subsubsection{Random Forest with Feature Engineering (RF-FE)}

\begin{table}[htbp]
\centering
\caption{Random Forest Hyperparameters}
\label{tab:rf_config}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Number of Trees & 300 \\
Maximum Depth & 20 \\
Minimum Samples Split & 5 \\
Minimum Samples Leaf & 2 \\
Random State & 42 \\
Parallel Jobs & 4 \\
Bootstrap & True \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Rationale}: These parameters balance model complexity with computational efficiency. The relatively deep trees (max depth = 20) allow capturing complex nonlinear relationships, while minimum sample constraints prevent overfitting.

\subsubsection{XGBoost with Feature Engineering (XGB-FE)}

\begin{table}[htbp]
\centering
\caption{XGBoost Hyperparameters}
\label{tab:xgb_config}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Number of Estimators & 200 \\
Maximum Depth & 6 \\
Learning Rate & 0.05 \\
Subsample Ratio & 0.8 \\
Column Sample by Tree & 0.8 \\
Random State & 42 \\
Parallel Jobs & 4 \\
Verbosity & 0 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Rationale}: Conservative learning rate (0.05) with moderate tree depth (6) and subsampling (0.8) to prevent overfitting. The 200 estimators provide sufficient boosting iterations while maintaining computational efficiency.

\subsubsection{LSTM with Feature Engineering (LSTM-FE)}

\begin{table}[htbp]
\centering
\caption{LSTM Architecture and Hyperparameters}
\label{tab:lstm_config}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\multicolumn{2}{l}{\textit{Architecture}} \\
LSTM Layer 1 Units & 64 \\
LSTM Layer 1 Activation & tanh \\
LSTM Layer 1 Return Sequences & True \\
Dropout Rate (Layer 1) & 0.2 \\
LSTM Layer 2 Units & 32 \\
LSTM Layer 2 Activation & tanh \\
Dense Layer Units & 16 \\
Dense Layer Activation & ReLU \\
Output Layer & 1 (linear) \\
\midrule
\multicolumn{2}{l}{\textit{Training}} \\
Optimizer & Adam \\
Learning Rate & 0.001 \\
Loss Function & MSE \\
Epochs & 100 \\
Batch Size & 16 \\
Early Stopping Patience & 10 \\
Parallel Jobs & 2 \\
\midrule
\multicolumn{2}{l}{\textit{Feature Selection}} \\
Maximum Features & 200 \\
Selection Method & Variance-based \\
Lookback Window & 4 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Rationale}: Two-layer LSTM architecture with decreasing units (64 â†’ 32) captures temporal dependencies at multiple scales. Dropout (0.2) and early stopping prevent overfitting. Feature limit (200) ensures computational feasibility while retaining most informative predictors.

\subsubsection{Hybrid RF-FE Model}

Our hybrid model combines the smart embedding strategy with additional optimizations:

\begin{table}[htbp]
\centering
\caption{Hybrid RF-FE Configuration}
\label{tab:hybrid_config}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Configuration} \\
\midrule
\multicolumn{2}{l}{\textit{Feature Engineering}} \\
Smart Embedding & Yes (504 lagged + 4,410 current) \\
Data Type & float32 \\
\midrule
\multicolumn{2}{l}{\textit{Feature Selection}} \\
SelectKBest Pre-screening & 500 features \\
Scoring Function & F-regression \\
3-Stage Selection & Yes \\
Final Features & $\sim$400--500 \\
\midrule
\multicolumn{2}{l}{\textit{Random Forest}} \\
Number of Trees & 200 \\
Maximum Depth & 12 \\
Random State & 42 \\
Parallel Jobs & 4 \\
\midrule
\multicolumn{2}{l}{\textit{Benchmarking}} \\
Random Walk Baseline & Yes \\
Relative RMSE Reporting & Yes \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Innovation}: The hybrid model adds SelectKBest pre-screening before the 3-stage selection, reducing computational cost while maintaining predictive power. Random Walk benchmarking provides interpretable performance metrics.

\subsection{Feature Selection Thresholds}

\begin{table}[htbp]
\centering
\caption{Feature Selection Threshold Values}
\label{tab:selection_thresholds}
\begin{tabular}{llr}
\toprule
\textbf{Stage} & \textbf{Threshold Type} & \textbf{Value} \\
\midrule
Stage 1 & Constant Variance & $10^{-6}$ \\
Stage 2 & Correlation & 0.95 \\
Stage 3 & Low Variance Percentile & 5\% \\
Hybrid & SelectKBest $k$ & 500 \\
LSTM & Maximum Features & 200 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Rolling Window Evaluation}

We employ an expanding window approach for out-of-sample evaluation:

\begin{algorithm}[H]
\caption{Rolling Window Forecast Evaluation}
\begin{algorithmic}[1]
\State \textbf{Input:} Data $Y$, training size $n_{\text{train}}$, test size $n_{\text{test}}$, horizon $h$
\For{$i = 1$ to $n_{\text{test}} - h$}
    \State $Y_{\text{train}} \gets Y[1 : n_{\text{train}} + i]$ \Comment{Expanding window}
    \State Apply feature engineering to $Y_{\text{train}}$
    \State Train model on engineered features
    \State $\hat{y}_{n_{\text{train}}+i+h} \gets$ Forecast for horizon $h$
    \State $y_{n_{\text{train}}+i+h} \gets$ Actual value
    \State Store forecast and actual
\EndFor
\State Calculate RMSE, MAE, and other metrics
\State \textbf{Output:} Forecast errors and performance metrics
\end{algorithmic}
\end{algorithm}

\textbf{Key Features}:
\begin{itemize}
    \item \textbf{Expanding Window}: Training set grows with each iteration, using all available historical data
    \item \textbf{No Look-Ahead Bias}: Feature engineering applied separately within each window
    \item \textbf{Realistic Evaluation}: Mimics real-time forecasting scenario
\end{itemize}

\subsection{Computational Environment}

\begin{table}[htbp]
\centering
\caption{Computational Specifications}
\label{tab:compute_specs}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
\multicolumn{2}{l}{\textit{Software}} \\
Python Version & 3.11+ \\
NumPy & 1.24+ \\
Pandas & 2.0+ \\
scikit-learn & 1.3+ \\
XGBoost & 2.0+ \\
TensorFlow & 2.13+ \\
joblib & 1.3+ \\
\midrule
\multicolumn{2}{l}{\textit{Parallelization}} \\
RF/XGB Workers & 4 \\
LSTM Workers & 2 \\
Batch Size & 20 forecasts \\
\midrule
\multicolumn{2}{l}{\textit{Memory Management}} \\
Data Type & float32 \\
Garbage Collection & Explicit \\
Expected Memory & 1--2 GB \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Performance Metrics}

We evaluate forecast performance using multiple metrics:

\subsubsection{Root Mean Squared Error (RMSE)}
\begin{equation}
    \text{RMSE} = \sqrt{\frac{1}{n}\sum_{t=1}^{n}(y_t - \hat{y}_t)^2}
\end{equation}

\subsubsection{Mean Absolute Error (MAE)}
\begin{equation}
    \text{MAE} = \frac{1}{n}\sum_{t=1}^{n}|y_t - \hat{y}_t|
\end{equation}

\subsubsection{Relative RMSE (Hybrid Model)}
\begin{equation}
    \text{Relative RMSE} = \frac{\text{RMSE}_{\text{Model}}}{\text{RMSE}_{\text{Random Walk}}}
\end{equation}

Values less than 1.0 indicate the model outperforms the naive Random Walk benchmark.

\subsection{Reproducibility}

To ensure reproducibility:
\begin{itemize}
    \item \textbf{Random Seeds}: Fixed at 42 for all models
    \item \textbf{Data Version}: FRED-MD vintage November 2025
    \item \textbf{Code Repository}: \url{https://github.com/NormidwitUpdatedII/48E-Shit}
    \item \textbf{Documentation}: Comprehensive README and inline comments
    \item \textbf{Version Control}: All changes tracked via Git
\end{itemize}

\subsection{Runtime Performance}

Table~\ref{tab:runtime} summarizes the computational efficiency of our optimized pipeline.

\begin{table}[htbp]
\centering
\caption{Runtime Performance (First Sample, All Horizons)}
\label{tab:runtime}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Before} & \textbf{After} & \textbf{Speedup} \\
\midrule
RF-FE & 80 hours & 4--8 hours & 10--20$\times$ \\
XGB-FE & 80 hours & 4--8 hours & 10--20$\times$ \\
LSTM-FE & 100+ hours & 8--12 hours & 10--15$\times$ \\
Hybrid RF-FE & -- & 4--8 hours & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Summary of Configurations}

Our experimental design features:
\begin{itemize}
    \item Two distinct samples (short-term: 2000--2025; long-term: 1959--2025)
    \item 12 forecast horizons (1--12 months)
    \item 2 target variables (CPI, PCE)
    \item Carefully tuned hyperparameters for each model
    \item Rigorous expanding window evaluation
    \item Comprehensive performance metrics including Random Walk benchmarking
    \item Full reproducibility with version-controlled code
\end{itemize}

This configuration ensures robust evaluation across different time periods, forecast horizons, and model specifications.
